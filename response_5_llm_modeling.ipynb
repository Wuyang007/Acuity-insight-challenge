{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Response_5: LLM-based prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of Contents\n",
    "- [1. Load Data](#1-load-data)  \n",
    "\n",
    "- [2. Help function](#2-help-function)  \n",
    "\n",
    "- [3. Preprocessing data](#3-preprocessing-data)\n",
    "\n",
    "- [4. Word embedding](#3-word-embedding)\n",
    "\n",
    "- [5. Latent structure transforming](#3-latent-structure-transforming)\n",
    "\n",
    "- [6. Classification on the transformed latent structure](#3-classification-on-the-transformed-latent-structure)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel, TrainingArguments, Trainer\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('datasets/feature_extraction.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>response</th>\n",
       "      <th>score</th>\n",
       "      <th>response_length</th>\n",
       "      <th>grammer_error</th>\n",
       "      <th>response_corrected</th>\n",
       "      <th>readability</th>\n",
       "      <th>type_token_ratio</th>\n",
       "      <th>mean_sentence_length</th>\n",
       "      <th>coherence</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>gpt_scores</th>\n",
       "      <th>Relevance</th>\n",
       "      <th>Completeness</th>\n",
       "      <th>Depth of Reflection</th>\n",
       "      <th>Response Quality</th>\n",
       "      <th>Linguistic Quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>During my journey, I faced a big health proble...</td>\n",
       "      <td>3</td>\n",
       "      <td>110</td>\n",
       "      <td>0</td>\n",
       "      <td>During my journey, I faced a big health proble...</td>\n",
       "      <td>78.28</td>\n",
       "      <td>0.647541</td>\n",
       "      <td>20.333333</td>\n",
       "      <td>0.487087</td>\n",
       "      <td>0.999572</td>\n",
       "      <td>{'Relevance': 'Yes', 'Completeness': 2, 'Depth...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>2</td>\n",
       "      <td>Moderate</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>i once had a big problem when i wasn't ready f...</td>\n",
       "      <td>3</td>\n",
       "      <td>70</td>\n",
       "      <td>8</td>\n",
       "      <td>I once had a big problem when I wasn't ready f...</td>\n",
       "      <td>91.11</td>\n",
       "      <td>0.765432</td>\n",
       "      <td>16.200000</td>\n",
       "      <td>0.321876</td>\n",
       "      <td>-0.986382</td>\n",
       "      <td>{'Relevance': 'Yes', 'Completeness': 2, 'Depth...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>2</td>\n",
       "      <td>Moderate</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>During my junior year I faced a significant ch...</td>\n",
       "      <td>3</td>\n",
       "      <td>135</td>\n",
       "      <td>2</td>\n",
       "      <td>During my junior year I faced a significant ch...</td>\n",
       "      <td>-65.56</td>\n",
       "      <td>0.678832</td>\n",
       "      <td>137.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.995565</td>\n",
       "      <td>{'Relevance': 'Yes', 'Completeness': 3, 'Depth...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>3</td>\n",
       "      <td>Moderate</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Once, I encountered a situation that was quite...</td>\n",
       "      <td>2</td>\n",
       "      <td>109</td>\n",
       "      <td>0</td>\n",
       "      <td>Once, I encountered a situation that was quite...</td>\n",
       "      <td>47.18</td>\n",
       "      <td>0.684211</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>0.308484</td>\n",
       "      <td>0.994910</td>\n",
       "      <td>{'Relevance': 'Yes', 'Completeness': 2, 'Depth...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>2</td>\n",
       "      <td>Moderate</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>during a trip, i suffered a serious accident t...</td>\n",
       "      <td>3</td>\n",
       "      <td>53</td>\n",
       "      <td>6</td>\n",
       "      <td>During a trip, I suffered a serious accident t...</td>\n",
       "      <td>49.52</td>\n",
       "      <td>0.721311</td>\n",
       "      <td>15.250000</td>\n",
       "      <td>0.434561</td>\n",
       "      <td>-0.989042</td>\n",
       "      <td>{'Relevance': 'Yes', 'Completeness': 2, 'Depth...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>2</td>\n",
       "      <td>Moderate</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                           response  score  \\\n",
       "0           0  During my journey, I faced a big health proble...      3   \n",
       "1           1  i once had a big problem when i wasn't ready f...      3   \n",
       "2           2  During my junior year I faced a significant ch...      3   \n",
       "3           3  Once, I encountered a situation that was quite...      2   \n",
       "4           4  during a trip, i suffered a serious accident t...      3   \n",
       "\n",
       "   response_length  grammer_error  \\\n",
       "0              110              0   \n",
       "1               70              8   \n",
       "2              135              2   \n",
       "3              109              0   \n",
       "4               53              6   \n",
       "\n",
       "                                  response_corrected  readability  \\\n",
       "0  During my journey, I faced a big health proble...        78.28   \n",
       "1  I once had a big problem when I wasn't ready f...        91.11   \n",
       "2  During my junior year I faced a significant ch...       -65.56   \n",
       "3  Once, I encountered a situation that was quite...        47.18   \n",
       "4  During a trip, I suffered a serious accident t...        49.52   \n",
       "\n",
       "   type_token_ratio  mean_sentence_length  coherence  sentiment  \\\n",
       "0          0.647541             20.333333   0.487087   0.999572   \n",
       "1          0.765432             16.200000   0.321876  -0.986382   \n",
       "2          0.678832            137.000000   0.000000   0.995565   \n",
       "3          0.684211             19.000000   0.308484   0.994910   \n",
       "4          0.721311             15.250000   0.434561  -0.989042   \n",
       "\n",
       "                                          gpt_scores Relevance  Completeness  \\\n",
       "0  {'Relevance': 'Yes', 'Completeness': 2, 'Depth...       Yes             2   \n",
       "1  {'Relevance': 'Yes', 'Completeness': 2, 'Depth...       Yes             2   \n",
       "2  {'Relevance': 'Yes', 'Completeness': 3, 'Depth...       Yes             3   \n",
       "3  {'Relevance': 'Yes', 'Completeness': 2, 'Depth...       Yes             2   \n",
       "4  {'Relevance': 'Yes', 'Completeness': 2, 'Depth...       Yes             2   \n",
       "\n",
       "  Depth of Reflection Response Quality Linguistic Quality  \n",
       "0            Moderate               No                 No  \n",
       "1            Moderate               No                 No  \n",
       "2            Moderate              Yes                 No  \n",
       "3            Moderate               No                 No  \n",
       "4            Moderate               No                 No  "
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Method 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Method 1: BERT-Based Text Classification**  \n",
    "\n",
    "1. Utilize a **pretrained BERT model** to generate embeddings for the response text, capturing contextual and semantic information.  \n",
    "2. Pass the embeddings through a **four-layer neural network** for classification, mapping the text representations to their corresponding scores.  \n",
    "3. Train the model using a supervised learning approach, optimizing for classification performance.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text, tokenizer):\n",
    "    # Tokenize the input text and get the input ids and attention masks\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bert_embeddings(text, tokenizer, model):\n",
    "    \"\"\"\n",
    "    This function generates the numerical representation of peice of text, which captures its meaning, using the BERT model\n",
    "    \"\"\"\n",
    "    inputs = tokenize_text(text, tokenizer)\n",
    "    \n",
    "    # Pass the tokenized input through BERT\n",
    "    with torch.no_grad():  # Disable gradient calculation to save memory\n",
    "        outputs = model(**inputs)\n",
    "        \n",
    "    # We use the embeddings from the [CLS] token (first token) as the document representation\n",
    "    embeddings = outputs.last_hidden_state[:, 0, :].squeeze()\n",
    "    \n",
    "    return embeddings.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Embedding the response: 100%|██████████| 1000/1000 [05:59<00:00,  2.78it/s]\n"
     ]
    }
   ],
   "source": [
    "embeddings = []\n",
    "for response in tqdm(df['response_corrected'], desc=\"Embedding the response\"):\n",
    "    embeddings.append(get_bert_embeddings(response, tokenizer, bert_model))\n",
    "\n",
    "df['embeddings'] = embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(df['embeddings'].tolist())  # Convert list of arrays into a 2D array\n",
    "y = np.array(df['score'])\n",
    "\n",
    "# Split data into train and test sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=21, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(800, 768)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScorePredictor(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes=5):\n",
    "        super(ScorePredictor, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 384)\n",
    "        self.fc2 = nn.Linear(384, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.fc4 = nn.Linear(64, num_classes)  # Output 5 logits (no activation here)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = self.fc4(x)  # Raw logits for classification\n",
    "        return x\n",
    "\n",
    "# Initialize the model\n",
    "input_dim = X_train.shape[1]  \n",
    "model = ScorePredictor(input_dim)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([800, 1])) that is different to the input size (torch.Size([800, 5])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10000], Loss: 1.74980521\n",
      "Epoch [20/10000], Loss: 1.62838209\n",
      "Epoch [30/10000], Loss: 1.06118965\n",
      "Epoch [40/10000], Loss: 0.83923447\n",
      "Epoch [50/10000], Loss: 0.70706666\n",
      "Epoch [60/10000], Loss: 0.64164537\n",
      "Epoch [70/10000], Loss: 0.60364807\n",
      "Epoch [80/10000], Loss: 0.56753349\n",
      "Epoch [90/10000], Loss: 0.53432864\n",
      "Epoch [100/10000], Loss: 0.50285804\n",
      "Epoch [110/10000], Loss: 0.47485527\n",
      "Epoch [120/10000], Loss: 0.44956717\n",
      "Epoch [130/10000], Loss: 0.42660120\n",
      "Epoch [140/10000], Loss: 0.40567324\n",
      "Epoch [150/10000], Loss: 0.38630584\n",
      "Epoch [160/10000], Loss: 0.36809438\n",
      "Epoch [170/10000], Loss: 0.35099956\n",
      "Epoch [180/10000], Loss: 0.33530530\n",
      "Epoch [190/10000], Loss: 0.32066971\n",
      "Epoch [200/10000], Loss: 0.30683622\n",
      "Epoch [210/10000], Loss: 0.29398561\n",
      "Epoch [220/10000], Loss: 0.28197607\n",
      "Epoch [230/10000], Loss: 0.27023998\n",
      "Epoch [240/10000], Loss: 0.25906238\n",
      "Epoch [250/10000], Loss: 0.24851002\n",
      "Epoch [260/10000], Loss: 0.23828527\n",
      "Epoch [270/10000], Loss: 0.22908299\n",
      "Epoch [280/10000], Loss: 0.22060549\n",
      "Epoch [290/10000], Loss: 0.21256618\n",
      "Epoch [300/10000], Loss: 0.20506343\n",
      "Epoch [310/10000], Loss: 0.19806847\n",
      "Epoch [320/10000], Loss: 0.19073424\n",
      "Epoch [330/10000], Loss: 0.18405165\n",
      "Epoch [340/10000], Loss: 0.17750742\n",
      "Epoch [350/10000], Loss: 0.17117009\n",
      "Epoch [360/10000], Loss: 0.16559558\n",
      "Epoch [370/10000], Loss: 0.15950701\n",
      "Epoch [380/10000], Loss: 0.15420760\n",
      "Epoch [390/10000], Loss: 0.14895242\n",
      "Epoch [400/10000], Loss: 0.14358354\n",
      "Epoch [410/10000], Loss: 0.13854362\n",
      "Epoch [420/10000], Loss: 0.13355228\n",
      "Epoch [430/10000], Loss: 0.12853265\n",
      "Epoch [440/10000], Loss: 0.12355284\n",
      "Epoch [450/10000], Loss: 0.11865848\n",
      "Epoch [460/10000], Loss: 0.11389057\n",
      "Epoch [470/10000], Loss: 0.10957561\n",
      "Epoch [480/10000], Loss: 0.10439190\n",
      "Epoch [490/10000], Loss: 0.10088599\n",
      "Epoch [500/10000], Loss: 0.09513455\n",
      "Epoch [510/10000], Loss: 0.09183847\n",
      "Epoch [520/10000], Loss: 0.08715201\n",
      "Epoch [530/10000], Loss: 0.08209209\n",
      "Epoch [540/10000], Loss: 0.07806837\n",
      "Epoch [550/10000], Loss: 0.07364743\n",
      "Epoch [560/10000], Loss: 0.07044473\n",
      "Epoch [570/10000], Loss: 0.06581910\n",
      "Epoch [580/10000], Loss: 0.06263543\n",
      "Epoch [590/10000], Loss: 0.05939886\n",
      "Epoch [600/10000], Loss: 0.05533487\n",
      "Epoch [610/10000], Loss: 0.05071690\n",
      "Epoch [620/10000], Loss: 0.04631648\n",
      "Epoch [630/10000], Loss: 0.04406570\n",
      "Epoch [640/10000], Loss: 0.04028083\n",
      "Epoch [650/10000], Loss: 0.04163283\n",
      "Epoch [660/10000], Loss: 0.03718993\n",
      "Epoch [670/10000], Loss: 0.03440197\n",
      "Epoch [680/10000], Loss: 0.03085447\n",
      "Epoch [690/10000], Loss: 0.03117637\n",
      "Epoch [700/10000], Loss: 0.02816476\n",
      "Epoch [710/10000], Loss: 0.02402954\n",
      "Epoch [720/10000], Loss: 0.02238806\n",
      "Epoch [730/10000], Loss: 0.02642312\n",
      "Epoch [740/10000], Loss: 0.02476939\n",
      "Epoch [750/10000], Loss: 0.01926583\n",
      "Epoch [760/10000], Loss: 0.01660337\n",
      "Epoch [770/10000], Loss: 0.01604083\n",
      "Epoch [780/10000], Loss: 0.02565148\n",
      "Epoch [790/10000], Loss: 0.02385743\n",
      "Epoch [800/10000], Loss: 0.01291053\n",
      "Epoch [810/10000], Loss: 0.01374400\n",
      "Epoch [820/10000], Loss: 0.01157054\n",
      "Epoch [830/10000], Loss: 0.01050699\n",
      "Epoch [840/10000], Loss: 0.00948202\n",
      "Epoch [850/10000], Loss: 0.00927479\n",
      "Epoch [860/10000], Loss: 0.00853208\n",
      "Epoch [870/10000], Loss: 0.01103397\n",
      "Epoch [880/10000], Loss: 0.01179485\n",
      "Epoch [890/10000], Loss: 0.01021833\n",
      "Epoch [900/10000], Loss: 0.00626510\n",
      "Epoch [910/10000], Loss: 0.00624690\n",
      "Epoch [920/10000], Loss: 0.00724642\n",
      "Epoch [930/10000], Loss: 0.00457754\n",
      "Epoch [940/10000], Loss: 0.00578158\n",
      "Epoch [950/10000], Loss: 0.00474379\n",
      "Epoch [960/10000], Loss: 0.00361975\n",
      "Epoch [970/10000], Loss: 0.00331205\n",
      "Epoch [980/10000], Loss: 0.00397543\n",
      "Epoch [990/10000], Loss: 0.00554435\n",
      "Epoch [1000/10000], Loss: 0.00275105\n",
      "Epoch [1010/10000], Loss: 0.00272583\n",
      "Epoch [1020/10000], Loss: 0.00389555\n",
      "Epoch [1030/10000], Loss: 0.00440695\n",
      "Epoch [1040/10000], Loss: 0.00354764\n",
      "Epoch [1050/10000], Loss: 0.00184821\n",
      "Epoch [1060/10000], Loss: 0.00673093\n",
      "Epoch [1070/10000], Loss: 0.00325383\n",
      "Epoch [1080/10000], Loss: 0.00223237\n",
      "Epoch [1090/10000], Loss: 0.00131703\n",
      "Epoch [1100/10000], Loss: 0.00212195\n",
      "Epoch [1110/10000], Loss: 0.00173989\n",
      "Epoch [1120/10000], Loss: 0.00782392\n",
      "Epoch [1130/10000], Loss: 0.00353199\n",
      "Epoch [1140/10000], Loss: 0.00238842\n",
      "Epoch [1150/10000], Loss: 0.00192800\n",
      "Epoch [1160/10000], Loss: 0.00104064\n",
      "Epoch [1170/10000], Loss: 0.00542085\n",
      "Epoch [1180/10000], Loss: 0.00193796\n",
      "Epoch [1190/10000], Loss: 0.00068998\n",
      "Epoch [1200/10000], Loss: 0.00217423\n",
      "Epoch [1210/10000], Loss: 0.00094940\n",
      "Epoch [1220/10000], Loss: 0.00560083\n",
      "Epoch [1230/10000], Loss: 0.00253538\n",
      "Epoch [1240/10000], Loss: 0.00136859\n",
      "Epoch [1250/10000], Loss: 0.00047263\n",
      "Epoch [1260/10000], Loss: 0.00120350\n",
      "Epoch [1270/10000], Loss: 0.00050906\n",
      "Epoch [1280/10000], Loss: 0.00061493\n",
      "Epoch [1290/10000], Loss: 0.02854191\n",
      "Epoch [1300/10000], Loss: 0.00219331\n",
      "Epoch [1310/10000], Loss: 0.00225921\n",
      "Epoch [1320/10000], Loss: 0.00126304\n",
      "Epoch [1330/10000], Loss: 0.00055603\n",
      "Epoch [1340/10000], Loss: 0.00028666\n",
      "Epoch [1350/10000], Loss: 0.00033074\n",
      "Epoch [1360/10000], Loss: 0.00024462\n",
      "Epoch [1370/10000], Loss: 0.00021927\n",
      "Epoch [1380/10000], Loss: 0.00021414\n",
      "Epoch [1390/10000], Loss: 0.00019185\n",
      "Epoch [1400/10000], Loss: 0.00025073\n",
      "Epoch [1410/10000], Loss: 0.00054401\n",
      "Epoch [1420/10000], Loss: 0.00084555\n",
      "Epoch [1430/10000], Loss: 0.00073963\n",
      "Epoch [1440/10000], Loss: 0.00084708\n",
      "Epoch [1450/10000], Loss: 0.00546358\n",
      "Epoch [1460/10000], Loss: 0.00104066\n",
      "Epoch [1470/10000], Loss: 0.00070240\n",
      "Epoch [1480/10000], Loss: 0.00021167\n",
      "Epoch [1490/10000], Loss: 0.00020271\n",
      "Epoch [1500/10000], Loss: 0.00031813\n",
      "Epoch [1510/10000], Loss: 0.00273008\n",
      "Epoch [1520/10000], Loss: 0.00160341\n",
      "Epoch [1530/10000], Loss: 0.00025038\n",
      "Epoch [1540/10000], Loss: 0.00016002\n",
      "Epoch [1550/10000], Loss: 0.00020611\n",
      "Epoch [1560/10000], Loss: 0.00028934\n",
      "Epoch [1570/10000], Loss: 0.00019632\n",
      "Epoch [1580/10000], Loss: 0.00027150\n",
      "Epoch [1590/10000], Loss: 0.00764207\n",
      "Epoch [1600/10000], Loss: 0.00836547\n",
      "Epoch [1610/10000], Loss: 0.00193394\n",
      "Epoch [1620/10000], Loss: 0.00053021\n",
      "Epoch [1630/10000], Loss: 0.00012221\n",
      "Epoch [1640/10000], Loss: 0.00016992\n",
      "Epoch [1650/10000], Loss: 0.00013636\n",
      "Epoch [1660/10000], Loss: 0.00007695\n",
      "Epoch [1670/10000], Loss: 0.00006256\n",
      "Epoch [1680/10000], Loss: 0.00005919\n",
      "Epoch [1690/10000], Loss: 0.00005368\n",
      "Epoch [1700/10000], Loss: 0.00005319\n",
      "Epoch [1710/10000], Loss: 0.00004902\n",
      "Epoch [1720/10000], Loss: 0.00005881\n",
      "Epoch [1730/10000], Loss: 0.00015754\n",
      "Epoch [1740/10000], Loss: 0.00114100\n",
      "Epoch [1750/10000], Loss: 0.00758534\n",
      "Epoch [1760/10000], Loss: 0.00264934\n",
      "Epoch [1770/10000], Loss: 0.00091775\n",
      "Epoch [1780/10000], Loss: 0.00031591\n",
      "Epoch [1790/10000], Loss: 0.00021275\n",
      "Epoch [1800/10000], Loss: 0.00010155\n",
      "Epoch [1810/10000], Loss: 0.00008115\n",
      "Epoch [1820/10000], Loss: 0.00004616\n",
      "Epoch [1830/10000], Loss: 0.00016053\n",
      "Epoch [1840/10000], Loss: 0.01110603\n",
      "Epoch [1850/10000], Loss: 0.01750524\n",
      "Epoch [1860/10000], Loss: 0.00102149\n",
      "Epoch [1870/10000], Loss: 0.00172001\n",
      "Epoch [1880/10000], Loss: 0.00068339\n",
      "Epoch [1890/10000], Loss: 0.00009469\n",
      "Epoch [1900/10000], Loss: 0.00006416\n",
      "Epoch [1910/10000], Loss: 0.00005295\n",
      "Epoch [1920/10000], Loss: 0.00004412\n",
      "Epoch [1930/10000], Loss: 0.00004063\n",
      "Epoch [1940/10000], Loss: 0.00003820\n",
      "Epoch [1950/10000], Loss: 0.00003417\n",
      "Epoch [1960/10000], Loss: 0.00003220\n",
      "Epoch [1970/10000], Loss: 0.00003031\n",
      "Epoch [1980/10000], Loss: 0.00002889\n",
      "Epoch [1990/10000], Loss: 0.00002772\n",
      "Epoch [2000/10000], Loss: 0.00002662\n",
      "Epoch [2010/10000], Loss: 0.00002574\n",
      "Epoch [2020/10000], Loss: 0.00002488\n",
      "Epoch [2030/10000], Loss: 0.00002434\n",
      "Epoch [2040/10000], Loss: 0.00002419\n",
      "Epoch [2050/10000], Loss: 0.00002580\n",
      "Epoch [2060/10000], Loss: 0.00003827\n",
      "Epoch [2070/10000], Loss: 0.00018731\n",
      "Epoch [2080/10000], Loss: 0.00315829\n",
      "Epoch [2090/10000], Loss: 0.00255095\n",
      "Epoch [2100/10000], Loss: 0.00133826\n",
      "Epoch [2110/10000], Loss: 0.00078383\n",
      "Epoch [2120/10000], Loss: 0.00007148\n",
      "Epoch [2130/10000], Loss: 0.00015737\n",
      "Epoch [2140/10000], Loss: 0.00008659\n",
      "Epoch [2150/10000], Loss: 0.00004798\n",
      "Epoch [2160/10000], Loss: 0.00002966\n",
      "Epoch [2170/10000], Loss: 0.00002101\n",
      "Epoch [2180/10000], Loss: 0.00002379\n",
      "Epoch [2190/10000], Loss: 0.00003474\n",
      "Epoch [2200/10000], Loss: 0.00021201\n",
      "Epoch [2210/10000], Loss: 0.00689111\n",
      "Epoch [2220/10000], Loss: 0.00746047\n",
      "Epoch [2230/10000], Loss: 0.00208723\n",
      "Epoch [2240/10000], Loss: 0.00117981\n",
      "Epoch [2250/10000], Loss: 0.00050518\n",
      "Epoch [2260/10000], Loss: 0.00014058\n",
      "Epoch [2270/10000], Loss: 0.00003357\n",
      "Epoch [2280/10000], Loss: 0.00004617\n",
      "Epoch [2290/10000], Loss: 0.00003624\n",
      "Epoch [2300/10000], Loss: 0.00002293\n",
      "Epoch [2310/10000], Loss: 0.00002275\n",
      "Epoch [2320/10000], Loss: 0.00002054\n",
      "Epoch [2330/10000], Loss: 0.00001922\n",
      "Epoch [2340/10000], Loss: 0.00001847\n",
      "Epoch [2350/10000], Loss: 0.00001789\n",
      "Epoch [2360/10000], Loss: 0.00001737\n",
      "Epoch [2370/10000], Loss: 0.00001692\n",
      "Epoch [2380/10000], Loss: 0.00001655\n",
      "Epoch [2390/10000], Loss: 0.00001651\n",
      "Epoch [2400/10000], Loss: 0.00001850\n",
      "Epoch [2410/10000], Loss: 0.00005766\n",
      "Epoch [2420/10000], Loss: 0.00153940\n",
      "Epoch [2430/10000], Loss: 0.02670974\n",
      "Epoch [2440/10000], Loss: 0.00850698\n",
      "Epoch [2450/10000], Loss: 0.00259204\n",
      "Epoch [2460/10000], Loss: 0.00057907\n",
      "Epoch [2470/10000], Loss: 0.00019502\n",
      "Epoch [2480/10000], Loss: 0.00012407\n",
      "Epoch [2490/10000], Loss: 0.00007939\n",
      "Epoch [2500/10000], Loss: 0.00003660\n",
      "Epoch [2510/10000], Loss: 0.00002085\n",
      "Epoch [2520/10000], Loss: 0.00002178\n",
      "Epoch [2530/10000], Loss: 0.00001804\n",
      "Epoch [2540/10000], Loss: 0.00001750\n",
      "Epoch [2550/10000], Loss: 0.00001653\n",
      "Epoch [2560/10000], Loss: 0.00001589\n",
      "Epoch [2570/10000], Loss: 0.00001541\n",
      "Epoch [2580/10000], Loss: 0.00001502\n",
      "Epoch [2590/10000], Loss: 0.00001466\n",
      "Epoch [2600/10000], Loss: 0.00001435\n",
      "Epoch [2610/10000], Loss: 0.00001408\n",
      "Epoch [2620/10000], Loss: 0.00001386\n",
      "Epoch [2630/10000], Loss: 0.00001412\n",
      "Epoch [2640/10000], Loss: 0.00002248\n",
      "Epoch [2650/10000], Loss: 0.00036293\n",
      "Epoch [2660/10000], Loss: 0.02753600\n",
      "Epoch [2670/10000], Loss: 0.00456739\n",
      "Epoch [2680/10000], Loss: 0.00059041\n",
      "Epoch [2690/10000], Loss: 0.00184375\n",
      "Epoch [2700/10000], Loss: 0.00071901\n",
      "Epoch [2710/10000], Loss: 0.00020100\n",
      "Epoch [2720/10000], Loss: 0.00008144\n",
      "Epoch [2730/10000], Loss: 0.00004841\n",
      "Epoch [2740/10000], Loss: 0.00003284\n",
      "Epoch [2750/10000], Loss: 0.00002243\n",
      "Epoch [2760/10000], Loss: 0.00001737\n",
      "Epoch [2770/10000], Loss: 0.00001650\n",
      "Epoch [2780/10000], Loss: 0.00001558\n",
      "Epoch [2790/10000], Loss: 0.00001484\n",
      "Epoch [2800/10000], Loss: 0.00001430\n",
      "Epoch [2810/10000], Loss: 0.00001388\n",
      "Epoch [2820/10000], Loss: 0.00001352\n",
      "Epoch [2830/10000], Loss: 0.00001320\n",
      "Epoch [2840/10000], Loss: 0.00001293\n",
      "Epoch [2850/10000], Loss: 0.00001269\n",
      "Epoch [2860/10000], Loss: 0.00001247\n",
      "Epoch [2870/10000], Loss: 0.00001227\n",
      "Epoch [2880/10000], Loss: 0.00001210\n",
      "Epoch [2890/10000], Loss: 0.00001205\n",
      "Epoch [2900/10000], Loss: 0.00001300\n",
      "Epoch [2910/10000], Loss: 0.00003640\n",
      "Epoch [2920/10000], Loss: 0.00115516\n",
      "Epoch [2930/10000], Loss: 0.04778048\n",
      "Epoch [2940/10000], Loss: 0.01022562\n",
      "Epoch [2950/10000], Loss: 0.00391304\n",
      "Epoch [2960/10000], Loss: 0.00054824\n",
      "Epoch [2970/10000], Loss: 0.00007380\n",
      "Epoch [2980/10000], Loss: 0.00003625\n",
      "Epoch [2990/10000], Loss: 0.00003085\n",
      "Epoch [3000/10000], Loss: 0.00003046\n",
      "Epoch [3010/10000], Loss: 0.00002586\n",
      "Epoch [3020/10000], Loss: 0.00001789\n",
      "Epoch [3030/10000], Loss: 0.00001465\n",
      "Epoch [3040/10000], Loss: 0.00001429\n",
      "Epoch [3050/10000], Loss: 0.00001325\n",
      "Epoch [3060/10000], Loss: 0.00001283\n",
      "Epoch [3070/10000], Loss: 0.00001240\n",
      "Epoch [3080/10000], Loss: 0.00001206\n",
      "Epoch [3090/10000], Loss: 0.00001178\n",
      "Epoch [3100/10000], Loss: 0.00001153\n",
      "Epoch [3110/10000], Loss: 0.00001131\n",
      "Epoch [3120/10000], Loss: 0.00001111\n",
      "Epoch [3130/10000], Loss: 0.00001094\n",
      "Epoch [3140/10000], Loss: 0.00001078\n",
      "Epoch [3150/10000], Loss: 0.00001064\n",
      "Epoch [3160/10000], Loss: 0.00001057\n",
      "Epoch [3170/10000], Loss: 0.00001195\n",
      "Epoch [3180/10000], Loss: 0.00008148\n",
      "Epoch [3190/10000], Loss: 0.00667380\n",
      "Epoch [3200/10000], Loss: 0.01892994\n",
      "Epoch [3210/10000], Loss: 0.01013971\n",
      "Epoch [3220/10000], Loss: 0.00052914\n",
      "Epoch [3230/10000], Loss: 0.00128052\n",
      "Epoch [3240/10000], Loss: 0.00011058\n",
      "Epoch [3250/10000], Loss: 0.00007108\n",
      "Epoch [3260/10000], Loss: 0.00007277\n",
      "Epoch [3270/10000], Loss: 0.00004142\n",
      "Epoch [3280/10000], Loss: 0.00002468\n",
      "Epoch [3290/10000], Loss: 0.00001816\n",
      "Epoch [3300/10000], Loss: 0.00001537\n",
      "Epoch [3310/10000], Loss: 0.00001383\n",
      "Epoch [3320/10000], Loss: 0.00001285\n",
      "Epoch [3330/10000], Loss: 0.00001222\n",
      "Epoch [3340/10000], Loss: 0.00001177\n",
      "Epoch [3350/10000], Loss: 0.00001138\n",
      "Epoch [3360/10000], Loss: 0.00001106\n",
      "Epoch [3370/10000], Loss: 0.00001078\n",
      "Epoch [3380/10000], Loss: 0.00001054\n",
      "Epoch [3390/10000], Loss: 0.00001032\n",
      "Epoch [3400/10000], Loss: 0.00001014\n",
      "Epoch [3410/10000], Loss: 0.00000997\n",
      "Epoch [3420/10000], Loss: 0.00000982\n",
      "Epoch [3430/10000], Loss: 0.00000968\n",
      "Epoch [3440/10000], Loss: 0.00000955\n",
      "Epoch [3450/10000], Loss: 0.00000943\n",
      "Epoch [3460/10000], Loss: 0.00000932\n",
      "Epoch [3470/10000], Loss: 0.00000922\n",
      "Epoch [3480/10000], Loss: 0.00000914\n",
      "Epoch [3490/10000], Loss: 0.00000922\n",
      "Epoch [3500/10000], Loss: 0.00001396\n",
      "Epoch [3510/10000], Loss: 0.00028478\n",
      "Epoch [3520/10000], Loss: 0.02747723\n",
      "Epoch [3530/10000], Loss: 0.00202104\n",
      "Epoch [3540/10000], Loss: 0.00327219\n",
      "Epoch [3550/10000], Loss: 0.00202281\n",
      "Epoch [3560/10000], Loss: 0.00008698\n",
      "Epoch [3570/10000], Loss: 0.00004349\n",
      "Epoch [3580/10000], Loss: 0.00004153\n",
      "Epoch [3590/10000], Loss: 0.00002875\n",
      "Epoch [3600/10000], Loss: 0.00001967\n",
      "Epoch [3610/10000], Loss: 0.00001508\n",
      "Epoch [3620/10000], Loss: 0.00001293\n",
      "Epoch [3630/10000], Loss: 0.00001194\n",
      "Epoch [3640/10000], Loss: 0.00001131\n",
      "Epoch [3650/10000], Loss: 0.00001072\n",
      "Epoch [3660/10000], Loss: 0.00001028\n",
      "Epoch [3670/10000], Loss: 0.00000994\n",
      "Epoch [3680/10000], Loss: 0.00000966\n",
      "Epoch [3690/10000], Loss: 0.00000941\n",
      "Epoch [3700/10000], Loss: 0.00000920\n",
      "Epoch [3710/10000], Loss: 0.00000902\n",
      "Epoch [3720/10000], Loss: 0.00000886\n",
      "Epoch [3730/10000], Loss: 0.00000872\n",
      "Epoch [3740/10000], Loss: 0.00000859\n",
      "Epoch [3750/10000], Loss: 0.00000848\n",
      "Epoch [3760/10000], Loss: 0.00000837\n",
      "Epoch [3770/10000], Loss: 0.00000828\n",
      "Epoch [3780/10000], Loss: 0.00000819\n",
      "Epoch [3790/10000], Loss: 0.00000815\n",
      "Epoch [3800/10000], Loss: 0.00000882\n",
      "Epoch [3810/10000], Loss: 0.00003283\n",
      "Epoch [3820/10000], Loss: 0.00161853\n",
      "Epoch [3830/10000], Loss: 0.03195375\n",
      "Epoch [3840/10000], Loss: 0.00968888\n",
      "Epoch [3850/10000], Loss: 0.00171582\n",
      "Epoch [3860/10000], Loss: 0.00009287\n",
      "Epoch [3870/10000], Loss: 0.00004285\n",
      "Epoch [3880/10000], Loss: 0.00002666\n",
      "Epoch [3890/10000], Loss: 0.00001746\n",
      "Epoch [3900/10000], Loss: 0.00001809\n",
      "Epoch [3910/10000], Loss: 0.00001825\n",
      "Epoch [3920/10000], Loss: 0.00001390\n",
      "Epoch [3930/10000], Loss: 0.00001041\n",
      "Epoch [3940/10000], Loss: 0.00001003\n",
      "Epoch [3950/10000], Loss: 0.00000937\n",
      "Epoch [3960/10000], Loss: 0.00000902\n",
      "Epoch [3970/10000], Loss: 0.00000869\n",
      "Epoch [3980/10000], Loss: 0.00000847\n",
      "Epoch [3990/10000], Loss: 0.00000827\n",
      "Epoch [4000/10000], Loss: 0.00000810\n",
      "Epoch [4010/10000], Loss: 0.00000796\n",
      "Epoch [4020/10000], Loss: 0.00000783\n",
      "Epoch [4030/10000], Loss: 0.00000771\n",
      "Epoch [4040/10000], Loss: 0.00000761\n",
      "Epoch [4050/10000], Loss: 0.00000752\n",
      "Epoch [4060/10000], Loss: 0.00000743\n",
      "Epoch [4070/10000], Loss: 0.00000738\n",
      "Epoch [4080/10000], Loss: 0.00000786\n",
      "Epoch [4090/10000], Loss: 0.00003161\n",
      "Epoch [4100/10000], Loss: 0.00213976\n",
      "Epoch [4110/10000], Loss: 0.02285094\n",
      "Epoch [4120/10000], Loss: 0.00073933\n",
      "Epoch [4130/10000], Loss: 0.00363905\n",
      "Epoch [4140/10000], Loss: 0.00073679\n",
      "Epoch [4150/10000], Loss: 0.00008753\n",
      "Epoch [4160/10000], Loss: 0.00018212\n",
      "Epoch [4170/10000], Loss: 0.00009240\n",
      "Epoch [4180/10000], Loss: 0.00003593\n",
      "Epoch [4190/10000], Loss: 0.00001827\n",
      "Epoch [4200/10000], Loss: 0.00001319\n",
      "Epoch [4210/10000], Loss: 0.00001120\n",
      "Epoch [4220/10000], Loss: 0.00001006\n",
      "Epoch [4230/10000], Loss: 0.00000927\n",
      "Epoch [4240/10000], Loss: 0.00000873\n",
      "Epoch [4250/10000], Loss: 0.00000839\n",
      "Epoch [4260/10000], Loss: 0.00000812\n",
      "Epoch [4270/10000], Loss: 0.00000789\n",
      "Epoch [4280/10000], Loss: 0.00000769\n",
      "Epoch [4290/10000], Loss: 0.00000753\n",
      "Epoch [4300/10000], Loss: 0.00000738\n",
      "Epoch [4310/10000], Loss: 0.00000725\n",
      "Epoch [4320/10000], Loss: 0.00000714\n",
      "Epoch [4330/10000], Loss: 0.00000704\n",
      "Epoch [4340/10000], Loss: 0.00000695\n",
      "Epoch [4350/10000], Loss: 0.00000686\n",
      "Epoch [4360/10000], Loss: 0.00000678\n",
      "Epoch [4370/10000], Loss: 0.00000671\n",
      "Epoch [4380/10000], Loss: 0.00000664\n",
      "Epoch [4390/10000], Loss: 0.00000658\n",
      "Epoch [4400/10000], Loss: 0.00000652\n",
      "Epoch [4410/10000], Loss: 0.00000652\n",
      "Epoch [4420/10000], Loss: 0.00000885\n",
      "Epoch [4430/10000], Loss: 0.00023040\n",
      "Epoch [4440/10000], Loss: 0.03136455\n",
      "Epoch [4450/10000], Loss: 0.00058242\n",
      "Epoch [4460/10000], Loss: 0.00704542\n",
      "Epoch [4470/10000], Loss: 0.00018372\n",
      "Epoch [4480/10000], Loss: 0.00099400\n",
      "Epoch [4490/10000], Loss: 0.00016673\n",
      "Epoch [4500/10000], Loss: 0.00003592\n",
      "Epoch [4510/10000], Loss: 0.00005432\n",
      "Epoch [4520/10000], Loss: 0.00002867\n",
      "Epoch [4530/10000], Loss: 0.00001483\n",
      "Epoch [4540/10000], Loss: 0.00001087\n",
      "Epoch [4550/10000], Loss: 0.00000957\n",
      "Epoch [4560/10000], Loss: 0.00000888\n",
      "Epoch [4570/10000], Loss: 0.00000839\n",
      "Epoch [4580/10000], Loss: 0.00000800\n",
      "Epoch [4590/10000], Loss: 0.00000769\n",
      "Epoch [4600/10000], Loss: 0.00000743\n",
      "Epoch [4610/10000], Loss: 0.00000722\n",
      "Epoch [4620/10000], Loss: 0.00000703\n",
      "Epoch [4630/10000], Loss: 0.00000688\n",
      "Epoch [4640/10000], Loss: 0.00000674\n",
      "Epoch [4650/10000], Loss: 0.00000662\n",
      "Epoch [4660/10000], Loss: 0.00000652\n",
      "Epoch [4670/10000], Loss: 0.00000642\n",
      "Epoch [4680/10000], Loss: 0.00000633\n",
      "Epoch [4690/10000], Loss: 0.00000625\n",
      "Epoch [4700/10000], Loss: 0.00000617\n",
      "Epoch [4710/10000], Loss: 0.00000610\n",
      "Epoch [4720/10000], Loss: 0.00000604\n",
      "Epoch [4730/10000], Loss: 0.00000598\n",
      "Epoch [4740/10000], Loss: 0.00000592\n",
      "Epoch [4750/10000], Loss: 0.00000587\n",
      "Epoch [4760/10000], Loss: 0.00000582\n",
      "Epoch [4770/10000], Loss: 0.00000577\n",
      "Epoch [4780/10000], Loss: 0.00000604\n",
      "Epoch [4790/10000], Loss: 0.00004102\n",
      "Epoch [4800/10000], Loss: 0.00681972\n",
      "Epoch [4810/10000], Loss: 0.03146468\n",
      "Epoch [4820/10000], Loss: 0.00442075\n",
      "Epoch [4830/10000], Loss: 0.00337749\n",
      "Epoch [4840/10000], Loss: 0.00012480\n",
      "Epoch [4850/10000], Loss: 0.00052590\n",
      "Epoch [4860/10000], Loss: 0.00006204\n",
      "Epoch [4870/10000], Loss: 0.00004473\n",
      "Epoch [4880/10000], Loss: 0.00003683\n",
      "Epoch [4890/10000], Loss: 0.00001485\n",
      "Epoch [4900/10000], Loss: 0.00001057\n",
      "Epoch [4910/10000], Loss: 0.00000980\n",
      "Epoch [4920/10000], Loss: 0.00000897\n",
      "Epoch [4930/10000], Loss: 0.00000828\n",
      "Epoch [4940/10000], Loss: 0.00000778\n",
      "Epoch [4950/10000], Loss: 0.00000740\n",
      "Epoch [4960/10000], Loss: 0.00000711\n",
      "Epoch [4970/10000], Loss: 0.00000687\n",
      "Epoch [4980/10000], Loss: 0.00000667\n",
      "Epoch [4990/10000], Loss: 0.00000650\n",
      "Epoch [5000/10000], Loss: 0.00000635\n",
      "Epoch [5010/10000], Loss: 0.00000622\n",
      "Epoch [5020/10000], Loss: 0.00000611\n",
      "Epoch [5030/10000], Loss: 0.00000601\n",
      "Epoch [5040/10000], Loss: 0.00000591\n",
      "Epoch [5050/10000], Loss: 0.00000583\n",
      "Epoch [5060/10000], Loss: 0.00000575\n",
      "Epoch [5070/10000], Loss: 0.00000568\n",
      "Epoch [5080/10000], Loss: 0.00000562\n",
      "Epoch [5090/10000], Loss: 0.00000556\n",
      "Epoch [5100/10000], Loss: 0.00000550\n",
      "Epoch [5110/10000], Loss: 0.00000544\n",
      "Epoch [5120/10000], Loss: 0.00000539\n",
      "Epoch [5130/10000], Loss: 0.00000535\n",
      "Epoch [5140/10000], Loss: 0.00000530\n",
      "Epoch [5150/10000], Loss: 0.00000526\n",
      "Epoch [5160/10000], Loss: 0.00000523\n",
      "Epoch [5170/10000], Loss: 0.00000664\n",
      "Epoch [5180/10000], Loss: 0.00021728\n",
      "Epoch [5190/10000], Loss: 0.04788101\n",
      "Epoch [5200/10000], Loss: 0.00761733\n",
      "Epoch [5210/10000], Loss: 0.00470782\n",
      "Epoch [5220/10000], Loss: 0.00188769\n",
      "Epoch [5230/10000], Loss: 0.00025773\n",
      "Epoch [5240/10000], Loss: 0.00026952\n",
      "Epoch [5250/10000], Loss: 0.00009254\n",
      "Epoch [5260/10000], Loss: 0.00001779\n",
      "Epoch [5270/10000], Loss: 0.00002551\n",
      "Epoch [5280/10000], Loss: 0.00001425\n",
      "Epoch [5290/10000], Loss: 0.00000917\n",
      "Epoch [5300/10000], Loss: 0.00000806\n",
      "Epoch [5310/10000], Loss: 0.00000750\n",
      "Epoch [5320/10000], Loss: 0.00000704\n",
      "Epoch [5330/10000], Loss: 0.00000668\n",
      "Epoch [5340/10000], Loss: 0.00000640\n",
      "Epoch [5350/10000], Loss: 0.00000617\n",
      "Epoch [5360/10000], Loss: 0.00000598\n",
      "Epoch [5370/10000], Loss: 0.00000582\n",
      "Epoch [5380/10000], Loss: 0.00000569\n",
      "Epoch [5390/10000], Loss: 0.00000557\n",
      "Epoch [5400/10000], Loss: 0.00000546\n",
      "Epoch [5410/10000], Loss: 0.00000537\n",
      "Epoch [5420/10000], Loss: 0.00000529\n",
      "Epoch [5430/10000], Loss: 0.00000521\n",
      "Epoch [5440/10000], Loss: 0.00000514\n",
      "Epoch [5450/10000], Loss: 0.00000508\n",
      "Epoch [5460/10000], Loss: 0.00000502\n",
      "Epoch [5470/10000], Loss: 0.00000496\n",
      "Epoch [5480/10000], Loss: 0.00000491\n",
      "Epoch [5490/10000], Loss: 0.00000487\n",
      "Epoch [5500/10000], Loss: 0.00000482\n",
      "Epoch [5510/10000], Loss: 0.00000478\n",
      "Epoch [5520/10000], Loss: 0.00000474\n",
      "Epoch [5530/10000], Loss: 0.00000470\n",
      "Epoch [5540/10000], Loss: 0.00000466\n",
      "Epoch [5550/10000], Loss: 0.00000471\n",
      "Epoch [5560/10000], Loss: 0.00000956\n",
      "Epoch [5570/10000], Loss: 0.00064386\n",
      "Epoch [5580/10000], Loss: 0.06930721\n",
      "Epoch [5590/10000], Loss: 0.01481073\n",
      "Epoch [5600/10000], Loss: 0.00135247\n",
      "Epoch [5610/10000], Loss: 0.00150822\n",
      "Epoch [5620/10000], Loss: 0.00014647\n",
      "Epoch [5630/10000], Loss: 0.00027846\n",
      "Epoch [5640/10000], Loss: 0.00003288\n",
      "Epoch [5650/10000], Loss: 0.00001881\n",
      "Epoch [5660/10000], Loss: 0.00001995\n",
      "Epoch [5670/10000], Loss: 0.00001233\n",
      "Epoch [5680/10000], Loss: 0.00000848\n",
      "Epoch [5690/10000], Loss: 0.00000712\n",
      "Epoch [5700/10000], Loss: 0.00000652\n",
      "Epoch [5710/10000], Loss: 0.00000615\n",
      "Epoch [5720/10000], Loss: 0.00000586\n",
      "Epoch [5730/10000], Loss: 0.00000563\n",
      "Epoch [5740/10000], Loss: 0.00000545\n",
      "Epoch [5750/10000], Loss: 0.00000529\n",
      "Epoch [5760/10000], Loss: 0.00000517\n",
      "Epoch [5770/10000], Loss: 0.00000505\n",
      "Epoch [5780/10000], Loss: 0.00000495\n",
      "Epoch [5790/10000], Loss: 0.00000487\n",
      "Epoch [5800/10000], Loss: 0.00000479\n",
      "Epoch [5810/10000], Loss: 0.00000472\n",
      "Epoch [5820/10000], Loss: 0.00000466\n",
      "Epoch [5830/10000], Loss: 0.00000460\n",
      "Epoch [5840/10000], Loss: 0.00000454\n",
      "Epoch [5850/10000], Loss: 0.00000449\n",
      "Epoch [5860/10000], Loss: 0.00000445\n",
      "Epoch [5870/10000], Loss: 0.00000440\n",
      "Epoch [5880/10000], Loss: 0.00000436\n",
      "Epoch [5890/10000], Loss: 0.00000431\n",
      "Epoch [5900/10000], Loss: 0.00000428\n",
      "Epoch [5910/10000], Loss: 0.00000426\n",
      "Epoch [5920/10000], Loss: 0.00000457\n",
      "Epoch [5930/10000], Loss: 0.00001894\n",
      "Epoch [5940/10000], Loss: 0.00136539\n",
      "Epoch [5950/10000], Loss: 0.03473172\n",
      "Epoch [5960/10000], Loss: 0.00921877\n",
      "Epoch [5970/10000], Loss: 0.00015296\n",
      "Epoch [5980/10000], Loss: 0.00086586\n",
      "Epoch [5990/10000], Loss: 0.00053166\n",
      "Epoch [6000/10000], Loss: 0.00014797\n",
      "Epoch [6010/10000], Loss: 0.00003533\n",
      "Epoch [6020/10000], Loss: 0.00001336\n",
      "Epoch [6030/10000], Loss: 0.00000892\n",
      "Epoch [6040/10000], Loss: 0.00000755\n",
      "Epoch [6050/10000], Loss: 0.00000677\n",
      "Epoch [6060/10000], Loss: 0.00000604\n",
      "Epoch [6070/10000], Loss: 0.00000543\n",
      "Epoch [6080/10000], Loss: 0.00000510\n",
      "Epoch [6090/10000], Loss: 0.00000493\n",
      "Epoch [6100/10000], Loss: 0.00000476\n",
      "Epoch [6110/10000], Loss: 0.00000464\n",
      "Epoch [6120/10000], Loss: 0.00000453\n",
      "Epoch [6130/10000], Loss: 0.00000443\n",
      "Epoch [6140/10000], Loss: 0.00000435\n",
      "Epoch [6150/10000], Loss: 0.00000427\n",
      "Epoch [6160/10000], Loss: 0.00000420\n",
      "Epoch [6170/10000], Loss: 0.00000414\n",
      "Epoch [6180/10000], Loss: 0.00000409\n",
      "Epoch [6190/10000], Loss: 0.00000404\n",
      "Epoch [6200/10000], Loss: 0.00000399\n",
      "Epoch [6210/10000], Loss: 0.00000395\n",
      "Epoch [6220/10000], Loss: 0.00000391\n",
      "Epoch [6230/10000], Loss: 0.00000387\n",
      "Epoch [6240/10000], Loss: 0.00000389\n",
      "Epoch [6250/10000], Loss: 0.00000569\n",
      "Epoch [6260/10000], Loss: 0.00015962\n",
      "Epoch [6270/10000], Loss: 0.02431229\n",
      "Epoch [6280/10000], Loss: 0.00572514\n",
      "Epoch [6290/10000], Loss: 0.00304843\n",
      "Epoch [6300/10000], Loss: 0.00193102\n",
      "Epoch [6310/10000], Loss: 0.00011502\n",
      "Epoch [6320/10000], Loss: 0.00033188\n",
      "Epoch [6330/10000], Loss: 0.00006366\n",
      "Epoch [6340/10000], Loss: 0.00001075\n",
      "Epoch [6350/10000], Loss: 0.00001539\n",
      "Epoch [6360/10000], Loss: 0.00001171\n",
      "Epoch [6370/10000], Loss: 0.00000794\n",
      "Epoch [6380/10000], Loss: 0.00000620\n",
      "Epoch [6390/10000], Loss: 0.00000544\n",
      "Epoch [6400/10000], Loss: 0.00000503\n",
      "Epoch [6410/10000], Loss: 0.00000476\n",
      "Epoch [6420/10000], Loss: 0.00000456\n",
      "Epoch [6430/10000], Loss: 0.00000442\n",
      "Epoch [6440/10000], Loss: 0.00000430\n",
      "Epoch [6450/10000], Loss: 0.00000420\n",
      "Epoch [6460/10000], Loss: 0.00000411\n",
      "Epoch [6470/10000], Loss: 0.00000403\n",
      "Epoch [6480/10000], Loss: 0.00000396\n",
      "Epoch [6490/10000], Loss: 0.00000390\n",
      "Epoch [6500/10000], Loss: 0.00000384\n",
      "Epoch [6510/10000], Loss: 0.00000379\n",
      "Epoch [6520/10000], Loss: 0.00000374\n",
      "Epoch [6530/10000], Loss: 0.00000370\n",
      "Epoch [6540/10000], Loss: 0.00000366\n",
      "Epoch [6550/10000], Loss: 0.00000362\n",
      "Epoch [6560/10000], Loss: 0.00000359\n",
      "Epoch [6570/10000], Loss: 0.00000355\n",
      "Epoch [6580/10000], Loss: 0.00000352\n",
      "Epoch [6590/10000], Loss: 0.00000349\n",
      "Epoch [6600/10000], Loss: 0.00000346\n",
      "Epoch [6610/10000], Loss: 0.00000343\n",
      "Epoch [6620/10000], Loss: 0.00000350\n",
      "Epoch [6630/10000], Loss: 0.00001380\n",
      "Epoch [6640/10000], Loss: 0.00227716\n",
      "Epoch [6650/10000], Loss: 0.00063888\n",
      "Epoch [6660/10000], Loss: 0.01309414\n",
      "Epoch [6670/10000], Loss: 0.00331174\n",
      "Epoch [6680/10000], Loss: 0.00009357\n",
      "Epoch [6690/10000], Loss: 0.00041261\n",
      "Epoch [6700/10000], Loss: 0.00022834\n",
      "Epoch [6710/10000], Loss: 0.00001573\n",
      "Epoch [6720/10000], Loss: 0.00003790\n",
      "Epoch [6730/10000], Loss: 0.00001059\n",
      "Epoch [6740/10000], Loss: 0.00001085\n",
      "Epoch [6750/10000], Loss: 0.00000683\n",
      "Epoch [6760/10000], Loss: 0.00000658\n",
      "Epoch [6770/10000], Loss: 0.00000567\n",
      "Epoch [6780/10000], Loss: 0.00000525\n",
      "Epoch [6790/10000], Loss: 0.00000496\n",
      "Epoch [6800/10000], Loss: 0.00000469\n",
      "Epoch [6810/10000], Loss: 0.00000448\n",
      "Epoch [6820/10000], Loss: 0.00000430\n",
      "Epoch [6830/10000], Loss: 0.00000415\n",
      "Epoch [6840/10000], Loss: 0.00000403\n",
      "Epoch [6850/10000], Loss: 0.00000393\n",
      "Epoch [6860/10000], Loss: 0.00000383\n",
      "Epoch [6870/10000], Loss: 0.00000375\n",
      "Epoch [6880/10000], Loss: 0.00000368\n",
      "Epoch [6890/10000], Loss: 0.00000362\n",
      "Epoch [6900/10000], Loss: 0.00000356\n",
      "Epoch [6910/10000], Loss: 0.00000351\n",
      "Epoch [6920/10000], Loss: 0.00000346\n",
      "Epoch [6930/10000], Loss: 0.00000341\n",
      "Epoch [6940/10000], Loss: 0.00000337\n",
      "Epoch [6950/10000], Loss: 0.00000333\n",
      "Epoch [6960/10000], Loss: 0.00000329\n",
      "Epoch [6970/10000], Loss: 0.00000326\n",
      "Epoch [6980/10000], Loss: 0.00000323\n",
      "Epoch [6990/10000], Loss: 0.00000320\n",
      "Epoch [7000/10000], Loss: 0.00000317\n",
      "Epoch [7010/10000], Loss: 0.00000314\n",
      "Epoch [7020/10000], Loss: 0.00000311\n",
      "Epoch [7030/10000], Loss: 0.00000308\n",
      "Epoch [7040/10000], Loss: 0.00000306\n",
      "Epoch [7050/10000], Loss: 0.00000303\n",
      "Epoch [7060/10000], Loss: 0.00000301\n",
      "Epoch [7070/10000], Loss: 0.00000305\n",
      "Epoch [7080/10000], Loss: 0.00000822\n",
      "Epoch [7090/10000], Loss: 0.00081064\n",
      "Epoch [7100/10000], Loss: 0.05196535\n",
      "Epoch [7110/10000], Loss: 0.00631925\n",
      "Epoch [7120/10000], Loss: 0.00285065\n",
      "Epoch [7130/10000], Loss: 0.00080749\n",
      "Epoch [7140/10000], Loss: 0.00031411\n",
      "Epoch [7150/10000], Loss: 0.00018152\n",
      "Epoch [7160/10000], Loss: 0.00001338\n",
      "Epoch [7170/10000], Loss: 0.00003189\n",
      "Epoch [7180/10000], Loss: 0.00001449\n",
      "Epoch [7190/10000], Loss: 0.00000638\n",
      "Epoch [7200/10000], Loss: 0.00000544\n",
      "Epoch [7210/10000], Loss: 0.00000505\n",
      "Epoch [7220/10000], Loss: 0.00000462\n",
      "Epoch [7230/10000], Loss: 0.00000428\n",
      "Epoch [7240/10000], Loss: 0.00000403\n",
      "Epoch [7250/10000], Loss: 0.00000383\n",
      "Epoch [7260/10000], Loss: 0.00000368\n",
      "Epoch [7270/10000], Loss: 0.00000355\n",
      "Epoch [7280/10000], Loss: 0.00000345\n",
      "Epoch [7290/10000], Loss: 0.00000335\n",
      "Epoch [7300/10000], Loss: 0.00000327\n",
      "Epoch [7310/10000], Loss: 0.00000320\n",
      "Epoch [7320/10000], Loss: 0.00000313\n",
      "Epoch [7330/10000], Loss: 0.00000307\n",
      "Epoch [7340/10000], Loss: 0.00000302\n",
      "Epoch [7350/10000], Loss: 0.00000298\n",
      "Epoch [7360/10000], Loss: 0.00000293\n",
      "Epoch [7370/10000], Loss: 0.00000290\n",
      "Epoch [7380/10000], Loss: 0.00000286\n",
      "Epoch [7390/10000], Loss: 0.00000283\n",
      "Epoch [7400/10000], Loss: 0.00000280\n",
      "Epoch [7410/10000], Loss: 0.00000277\n",
      "Epoch [7420/10000], Loss: 0.00000274\n",
      "Epoch [7430/10000], Loss: 0.00000271\n",
      "Epoch [7440/10000], Loss: 0.00000269\n",
      "Epoch [7450/10000], Loss: 0.00000267\n",
      "Epoch [7460/10000], Loss: 0.00000289\n",
      "Epoch [7470/10000], Loss: 0.00002144\n",
      "Epoch [7480/10000], Loss: 0.00276607\n",
      "Epoch [7490/10000], Loss: 0.00046238\n",
      "Epoch [7500/10000], Loss: 0.00778329\n",
      "Epoch [7510/10000], Loss: 0.00102949\n",
      "Epoch [7520/10000], Loss: 0.00115058\n",
      "Epoch [7530/10000], Loss: 0.00016563\n",
      "Epoch [7540/10000], Loss: 0.00008982\n",
      "Epoch [7550/10000], Loss: 0.00006396\n",
      "Epoch [7560/10000], Loss: 0.00000841\n",
      "Epoch [7570/10000], Loss: 0.00000884\n",
      "Epoch [7580/10000], Loss: 0.00000783\n",
      "Epoch [7590/10000], Loss: 0.00000549\n",
      "Epoch [7600/10000], Loss: 0.00000441\n",
      "Epoch [7610/10000], Loss: 0.00000397\n",
      "Epoch [7620/10000], Loss: 0.00000372\n",
      "Epoch [7630/10000], Loss: 0.00000353\n",
      "Epoch [7640/10000], Loss: 0.00000338\n",
      "Epoch [7650/10000], Loss: 0.00000326\n",
      "Epoch [7660/10000], Loss: 0.00000316\n",
      "Epoch [7670/10000], Loss: 0.00000307\n",
      "Epoch [7680/10000], Loss: 0.00000299\n",
      "Epoch [7690/10000], Loss: 0.00000293\n",
      "Epoch [7700/10000], Loss: 0.00000287\n",
      "Epoch [7710/10000], Loss: 0.00000282\n",
      "Epoch [7720/10000], Loss: 0.00000277\n",
      "Epoch [7730/10000], Loss: 0.00000272\n",
      "Epoch [7740/10000], Loss: 0.00000269\n",
      "Epoch [7750/10000], Loss: 0.00000265\n",
      "Epoch [7760/10000], Loss: 0.00000261\n",
      "Epoch [7770/10000], Loss: 0.00000258\n",
      "Epoch [7780/10000], Loss: 0.00000255\n",
      "Epoch [7790/10000], Loss: 0.00000253\n",
      "Epoch [7800/10000], Loss: 0.00000250\n",
      "Epoch [7810/10000], Loss: 0.00000247\n",
      "Epoch [7820/10000], Loss: 0.00000245\n",
      "Epoch [7830/10000], Loss: 0.00000243\n",
      "Epoch [7840/10000], Loss: 0.00000241\n",
      "Epoch [7850/10000], Loss: 0.00000266\n",
      "Epoch [7860/10000], Loss: 0.00002696\n",
      "Epoch [7870/10000], Loss: 0.00343902\n",
      "Epoch [7880/10000], Loss: 0.00303717\n",
      "Epoch [7890/10000], Loss: 0.00934404\n",
      "Epoch [7900/10000], Loss: 0.00075545\n",
      "Epoch [7910/10000], Loss: 0.00079716\n",
      "Epoch [7920/10000], Loss: 0.00027940\n",
      "Epoch [7930/10000], Loss: 0.00001948\n",
      "Epoch [7940/10000], Loss: 0.00005563\n",
      "Epoch [7950/10000], Loss: 0.00002042\n",
      "Epoch [7960/10000], Loss: 0.00000599\n",
      "Epoch [7970/10000], Loss: 0.00000456\n",
      "Epoch [7980/10000], Loss: 0.00000426\n",
      "Epoch [7990/10000], Loss: 0.00000383\n",
      "Epoch [8000/10000], Loss: 0.00000349\n",
      "Epoch [8010/10000], Loss: 0.00000326\n",
      "Epoch [8020/10000], Loss: 0.00000309\n",
      "Epoch [8030/10000], Loss: 0.00000296\n",
      "Epoch [8040/10000], Loss: 0.00000286\n",
      "Epoch [8050/10000], Loss: 0.00000277\n",
      "Epoch [8060/10000], Loss: 0.00000270\n",
      "Epoch [8070/10000], Loss: 0.00000263\n",
      "Epoch [8080/10000], Loss: 0.00000258\n",
      "Epoch [8090/10000], Loss: 0.00000253\n",
      "Epoch [8100/10000], Loss: 0.00000248\n",
      "Epoch [8110/10000], Loss: 0.00000244\n",
      "Epoch [8120/10000], Loss: 0.00000240\n",
      "Epoch [8130/10000], Loss: 0.00000237\n",
      "Epoch [8140/10000], Loss: 0.00000234\n",
      "Epoch [8150/10000], Loss: 0.00000231\n",
      "Epoch [8160/10000], Loss: 0.00000228\n",
      "Epoch [8170/10000], Loss: 0.00000225\n",
      "Epoch [8180/10000], Loss: 0.00000222\n",
      "Epoch [8190/10000], Loss: 0.00000220\n",
      "Epoch [8200/10000], Loss: 0.00000217\n",
      "Epoch [8210/10000], Loss: 0.00000215\n",
      "Epoch [8220/10000], Loss: 0.00000215\n",
      "Epoch [8230/10000], Loss: 0.00000287\n",
      "Epoch [8240/10000], Loss: 0.00006897\n",
      "Epoch [8250/10000], Loss: 0.01049399\n",
      "Epoch [8260/10000], Loss: 0.02536742\n",
      "Epoch [8270/10000], Loss: 0.00061384\n",
      "Epoch [8280/10000], Loss: 0.00235771\n",
      "Epoch [8290/10000], Loss: 0.00040823\n",
      "Epoch [8300/10000], Loss: 0.00022960\n",
      "Epoch [8310/10000], Loss: 0.00006335\n",
      "Epoch [8320/10000], Loss: 0.00004337\n",
      "Epoch [8330/10000], Loss: 0.00000649\n",
      "Epoch [8340/10000], Loss: 0.00001026\n",
      "Epoch [8350/10000], Loss: 0.00000552\n",
      "Epoch [8360/10000], Loss: 0.00000373\n",
      "Epoch [8370/10000], Loss: 0.00000346\n",
      "Epoch [8380/10000], Loss: 0.00000322\n",
      "Epoch [8390/10000], Loss: 0.00000299\n",
      "Epoch [8400/10000], Loss: 0.00000283\n",
      "Epoch [8410/10000], Loss: 0.00000270\n",
      "Epoch [8420/10000], Loss: 0.00000260\n",
      "Epoch [8430/10000], Loss: 0.00000251\n",
      "Epoch [8440/10000], Loss: 0.00000244\n",
      "Epoch [8450/10000], Loss: 0.00000238\n",
      "Epoch [8460/10000], Loss: 0.00000233\n",
      "Epoch [8470/10000], Loss: 0.00000229\n",
      "Epoch [8480/10000], Loss: 0.00000225\n",
      "Epoch [8490/10000], Loss: 0.00000221\n",
      "Epoch [8500/10000], Loss: 0.00000218\n",
      "Epoch [8510/10000], Loss: 0.00000215\n",
      "Epoch [8520/10000], Loss: 0.00000212\n",
      "Epoch [8530/10000], Loss: 0.00000209\n",
      "Epoch [8540/10000], Loss: 0.00000206\n",
      "Epoch [8550/10000], Loss: 0.00000204\n",
      "Epoch [8560/10000], Loss: 0.00000202\n",
      "Epoch [8570/10000], Loss: 0.00000200\n",
      "Epoch [8580/10000], Loss: 0.00000198\n",
      "Epoch [8590/10000], Loss: 0.00000196\n",
      "Epoch [8600/10000], Loss: 0.00000194\n",
      "Epoch [8610/10000], Loss: 0.00000192\n",
      "Epoch [8620/10000], Loss: 0.00000190\n",
      "Epoch [8630/10000], Loss: 0.00000188\n",
      "Epoch [8640/10000], Loss: 0.00000190\n",
      "Epoch [8650/10000], Loss: 0.00000543\n",
      "Epoch [8660/10000], Loss: 0.00070547\n",
      "Epoch [8670/10000], Loss: 0.05457255\n",
      "Epoch [8680/10000], Loss: 0.00045403\n",
      "Epoch [8690/10000], Loss: 0.00363315\n",
      "Epoch [8700/10000], Loss: 0.00193898\n",
      "Epoch [8710/10000], Loss: 0.00013541\n",
      "Epoch [8720/10000], Loss: 0.00010380\n",
      "Epoch [8730/10000], Loss: 0.00009041\n",
      "Epoch [8740/10000], Loss: 0.00000792\n",
      "Epoch [8750/10000], Loss: 0.00001615\n",
      "Epoch [8760/10000], Loss: 0.00000501\n",
      "Epoch [8770/10000], Loss: 0.00000553\n",
      "Epoch [8780/10000], Loss: 0.00000377\n",
      "Epoch [8790/10000], Loss: 0.00000349\n",
      "Epoch [8800/10000], Loss: 0.00000319\n",
      "Epoch [8810/10000], Loss: 0.00000294\n",
      "Epoch [8820/10000], Loss: 0.00000278\n",
      "Epoch [8830/10000], Loss: 0.00000266\n",
      "Epoch [8840/10000], Loss: 0.00000255\n",
      "Epoch [8850/10000], Loss: 0.00000246\n",
      "Epoch [8860/10000], Loss: 0.00000239\n",
      "Epoch [8870/10000], Loss: 0.00000232\n",
      "Epoch [8880/10000], Loss: 0.00000227\n",
      "Epoch [8890/10000], Loss: 0.00000222\n",
      "Epoch [8900/10000], Loss: 0.00000218\n",
      "Epoch [8910/10000], Loss: 0.00000214\n",
      "Epoch [8920/10000], Loss: 0.00000210\n",
      "Epoch [8930/10000], Loss: 0.00000207\n",
      "Epoch [8940/10000], Loss: 0.00000204\n",
      "Epoch [8950/10000], Loss: 0.00000201\n",
      "Epoch [8960/10000], Loss: 0.00000198\n",
      "Epoch [8970/10000], Loss: 0.00000196\n",
      "Epoch [8980/10000], Loss: 0.00000193\n",
      "Epoch [8990/10000], Loss: 0.00000191\n",
      "Epoch [9000/10000], Loss: 0.00000189\n",
      "Epoch [9010/10000], Loss: 0.00000187\n",
      "Epoch [9020/10000], Loss: 0.00000185\n",
      "Epoch [9030/10000], Loss: 0.00000183\n",
      "Epoch [9040/10000], Loss: 0.00000181\n",
      "Epoch [9050/10000], Loss: 0.00000180\n",
      "Epoch [9060/10000], Loss: 0.00000178\n",
      "Epoch [9070/10000], Loss: 0.00000176\n",
      "Epoch [9080/10000], Loss: 0.00000175\n",
      "Epoch [9090/10000], Loss: 0.00000175\n",
      "Epoch [9100/10000], Loss: 0.00000286\n",
      "Epoch [9110/10000], Loss: 0.00013580\n",
      "Epoch [9120/10000], Loss: 0.02523118\n",
      "Epoch [9130/10000], Loss: 0.00066924\n",
      "Epoch [9140/10000], Loss: 0.00525542\n",
      "Epoch [9150/10000], Loss: 0.00044035\n",
      "Epoch [9160/10000], Loss: 0.00050530\n",
      "Epoch [9170/10000], Loss: 0.00016280\n",
      "Epoch [9180/10000], Loss: 0.00001529\n",
      "Epoch [9190/10000], Loss: 0.00003566\n",
      "Epoch [9200/10000], Loss: 0.00001208\n",
      "Epoch [9210/10000], Loss: 0.00000371\n",
      "Epoch [9220/10000], Loss: 0.00000309\n",
      "Epoch [9230/10000], Loss: 0.00000289\n",
      "Epoch [9240/10000], Loss: 0.00000260\n",
      "Epoch [9250/10000], Loss: 0.00000238\n",
      "Epoch [9260/10000], Loss: 0.00000223\n",
      "Epoch [9270/10000], Loss: 0.00000212\n",
      "Epoch [9280/10000], Loss: 0.00000204\n",
      "Epoch [9290/10000], Loss: 0.00000198\n",
      "Epoch [9300/10000], Loss: 0.00000193\n",
      "Epoch [9310/10000], Loss: 0.00000188\n",
      "Epoch [9320/10000], Loss: 0.00000184\n",
      "Epoch [9330/10000], Loss: 0.00000181\n",
      "Epoch [9340/10000], Loss: 0.00000178\n",
      "Epoch [9350/10000], Loss: 0.00000175\n",
      "Epoch [9360/10000], Loss: 0.00000172\n",
      "Epoch [9370/10000], Loss: 0.00000170\n",
      "Epoch [9380/10000], Loss: 0.00000168\n",
      "Epoch [9390/10000], Loss: 0.00000165\n",
      "Epoch [9400/10000], Loss: 0.00000163\n",
      "Epoch [9410/10000], Loss: 0.00000162\n",
      "Epoch [9420/10000], Loss: 0.00000160\n",
      "Epoch [9430/10000], Loss: 0.00000158\n",
      "Epoch [9440/10000], Loss: 0.00000157\n",
      "Epoch [9450/10000], Loss: 0.00000155\n",
      "Epoch [9460/10000], Loss: 0.00000153\n",
      "Epoch [9470/10000], Loss: 0.00000152\n",
      "Epoch [9480/10000], Loss: 0.00000151\n",
      "Epoch [9490/10000], Loss: 0.00000182\n",
      "Epoch [9500/10000], Loss: 0.00005157\n",
      "Epoch [9510/10000], Loss: 0.01451148\n",
      "Epoch [9520/10000], Loss: 0.00695601\n",
      "Epoch [9530/10000], Loss: 0.01020408\n",
      "Epoch [9540/10000], Loss: 0.00298290\n",
      "Epoch [9550/10000], Loss: 0.00082320\n",
      "Epoch [9560/10000], Loss: 0.00007593\n",
      "Epoch [9570/10000], Loss: 0.00003423\n",
      "Epoch [9580/10000], Loss: 0.00005530\n",
      "Epoch [9590/10000], Loss: 0.00001839\n",
      "Epoch [9600/10000], Loss: 0.00000565\n",
      "Epoch [9610/10000], Loss: 0.00000672\n",
      "Epoch [9620/10000], Loss: 0.00000389\n",
      "Epoch [9630/10000], Loss: 0.00000367\n",
      "Epoch [9640/10000], Loss: 0.00000311\n",
      "Epoch [9650/10000], Loss: 0.00000290\n",
      "Epoch [9660/10000], Loss: 0.00000268\n",
      "Epoch [9670/10000], Loss: 0.00000253\n",
      "Epoch [9680/10000], Loss: 0.00000241\n",
      "Epoch [9690/10000], Loss: 0.00000231\n",
      "Epoch [9700/10000], Loss: 0.00000222\n",
      "Epoch [9710/10000], Loss: 0.00000215\n",
      "Epoch [9720/10000], Loss: 0.00000208\n",
      "Epoch [9730/10000], Loss: 0.00000203\n",
      "Epoch [9740/10000], Loss: 0.00000198\n",
      "Epoch [9750/10000], Loss: 0.00000193\n",
      "Epoch [9760/10000], Loss: 0.00000189\n",
      "Epoch [9770/10000], Loss: 0.00000186\n",
      "Epoch [9780/10000], Loss: 0.00000182\n",
      "Epoch [9790/10000], Loss: 0.00000179\n",
      "Epoch [9800/10000], Loss: 0.00000176\n",
      "Epoch [9810/10000], Loss: 0.00000174\n",
      "Epoch [9820/10000], Loss: 0.00000171\n",
      "Epoch [9830/10000], Loss: 0.00000169\n",
      "Epoch [9840/10000], Loss: 0.00000166\n",
      "Epoch [9850/10000], Loss: 0.00000164\n",
      "Epoch [9860/10000], Loss: 0.00000162\n",
      "Epoch [9870/10000], Loss: 0.00000160\n",
      "Epoch [9880/10000], Loss: 0.00000158\n",
      "Epoch [9890/10000], Loss: 0.00000157\n",
      "Epoch [9900/10000], Loss: 0.00000155\n",
      "Epoch [9910/10000], Loss: 0.00000153\n",
      "Epoch [9920/10000], Loss: 0.00000152\n",
      "Epoch [9930/10000], Loss: 0.00000150\n",
      "Epoch [9940/10000], Loss: 0.00000149\n",
      "Epoch [9950/10000], Loss: 0.00000147\n",
      "Epoch [9960/10000], Loss: 0.00000146\n",
      "Epoch [9970/10000], Loss: 0.00000150\n",
      "Epoch [9980/10000], Loss: 0.00000482\n",
      "Epoch [9990/10000], Loss: 0.00043846\n",
      "Epoch [10000/10000], Loss: 0.03684901\n"
     ]
    }
   ],
   "source": [
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()  # Mean Squared Error loss for regression task\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "epochs = 10000\n",
    "for epoch in range(epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    \n",
    "    # Zero the gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = model(X_train_tensor)\n",
    "    \n",
    "    # Compute the loss\n",
    "    loss = criterion(outputs, y_train_tensor)  # Targets should be integers (0-4)\n",
    "    \n",
    "    # Backward pass (compute gradients)\n",
    "    loss.backward()\n",
    "\n",
    "    # Update weights\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print loss every 10 epochs\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.8f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()  # Set the model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    # Forward pass to get logits\n",
    "    logits = model(X_test_tensor)\n",
    "\n",
    "    # Convert logits to probabilities using softmax\n",
    "    probabilities = torch.softmax(logits, dim=1)  # Apply softmax across the classes (dim=1)\n",
    "\n",
    "    # Get predicted labels by finding the index of the max probability\n",
    "    predicted_labels = torch.argmax(probabilities, dim=1)  # Predicted class indices\n",
    "\n",
    "    # Optionally, you can also get the class probabilities for the predicted labels\n",
    "    predicted_probabilities = probabilities.gather(1, predicted_labels.view(-1, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1,\n",
       "        0, 1, 1, 1, 1, 0, 1, 1, 0, 4, 0, 0, 4, 1, 1, 1, 1, 0, 0, 2, 1, 0, 0, 1,\n",
       "        1, 4, 1, 0, 1, 1, 1, 1, 1, 1, 1, 4, 0, 1, 4, 1, 1, 4, 1, 4, 1, 0, 4, 1,\n",
       "        1, 0, 1, 1, 1, 4, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,\n",
       "        4, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 0, 1, 1, 1, 0, 4, 0, 2, 0, 1, 1, 1, 1,\n",
       "        4, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        0, 2, 1, 1, 1, 0, 1, 1, 1, 4, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,\n",
       "        1, 1, 1, 0, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.200570</td>\n",
       "      <td>0.200594</td>\n",
       "      <td>0.200214</td>\n",
       "      <td>0.199589</td>\n",
       "      <td>0.199033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.201040</td>\n",
       "      <td>0.200768</td>\n",
       "      <td>0.199899</td>\n",
       "      <td>0.198942</td>\n",
       "      <td>0.199351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.200288</td>\n",
       "      <td>0.200370</td>\n",
       "      <td>0.199775</td>\n",
       "      <td>0.199260</td>\n",
       "      <td>0.200306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.199981</td>\n",
       "      <td>0.200982</td>\n",
       "      <td>0.199938</td>\n",
       "      <td>0.199331</td>\n",
       "      <td>0.199768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.199896</td>\n",
       "      <td>0.200698</td>\n",
       "      <td>0.199727</td>\n",
       "      <td>0.199623</td>\n",
       "      <td>0.200055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>0.200890</td>\n",
       "      <td>0.200384</td>\n",
       "      <td>0.200131</td>\n",
       "      <td>0.199308</td>\n",
       "      <td>0.199288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>0.200054</td>\n",
       "      <td>0.201394</td>\n",
       "      <td>0.199923</td>\n",
       "      <td>0.198954</td>\n",
       "      <td>0.199675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>0.199214</td>\n",
       "      <td>0.201213</td>\n",
       "      <td>0.199721</td>\n",
       "      <td>0.199541</td>\n",
       "      <td>0.200311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>0.199994</td>\n",
       "      <td>0.200424</td>\n",
       "      <td>0.200221</td>\n",
       "      <td>0.199693</td>\n",
       "      <td>0.199668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>0.200165</td>\n",
       "      <td>0.200759</td>\n",
       "      <td>0.199982</td>\n",
       "      <td>0.199777</td>\n",
       "      <td>0.199317</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4\n",
       "0    0.200570  0.200594  0.200214  0.199589  0.199033\n",
       "1    0.201040  0.200768  0.199899  0.198942  0.199351\n",
       "2    0.200288  0.200370  0.199775  0.199260  0.200306\n",
       "3    0.199981  0.200982  0.199938  0.199331  0.199768\n",
       "4    0.199896  0.200698  0.199727  0.199623  0.200055\n",
       "..        ...       ...       ...       ...       ...\n",
       "195  0.200890  0.200384  0.200131  0.199308  0.199288\n",
       "196  0.200054  0.201394  0.199923  0.198954  0.199675\n",
       "197  0.199214  0.201213  0.199721  0.199541  0.200311\n",
       "198  0.199994  0.200424  0.200221  0.199693  0.199668\n",
       "199  0.200165  0.200759  0.199982  0.199777  0.199317\n",
       "\n",
       "[200 rows x 5 columns]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(probabilities.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 4, 4, 2, 4, 4, 3, 3, 4, 3, 2, 5, 4, 4, 4, 4, 5, 5, 3, 3, 3, 5,\n",
       "       2, 4, 3, 4, 3, 4, 4, 3, 3, 3, 2, 3, 3, 3, 3, 5, 3, 4, 4, 3, 3, 3,\n",
       "       4, 2, 2, 3, 3, 4, 3, 5, 2, 5, 5, 5, 5, 5, 3, 4, 3, 3, 4, 4, 2, 5,\n",
       "       3, 3, 4, 3, 5, 5, 5, 2, 2, 4, 3, 3, 5, 2, 2, 5, 5, 4, 3, 4, 5, 5,\n",
       "       3, 2, 2, 3, 5, 3, 5, 5, 3, 2, 2, 2, 2, 3, 4, 3, 5, 2, 1, 2, 3, 4,\n",
       "       3, 5, 3, 3, 2, 3, 5, 5, 4, 4, 2, 3, 3, 4, 4, 4, 5, 3, 2, 4, 3, 2,\n",
       "       5, 3, 1, 2, 4, 3, 2, 4, 3, 3, 3, 3, 3, 3, 3, 3, 5, 4, 2, 3, 4, 3,\n",
       "       2, 4, 2, 3, 4, 2, 3, 3, 2, 4, 2, 4, 3, 4, 3, 5, 3, 2, 2, 4, 3, 5,\n",
       "       4, 3, 3, 4, 4, 4, 3, 2, 3, 2, 3, 3, 5, 5, 3, 5, 3, 4, 3, 1, 4, 5,\n",
       "       4, 3])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.035"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(predicted_labels, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Method 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Method 2: Fine-Tuning BERT for Text Classification**  \n",
    "\n",
    "1. **Initialize a pretrained BERT model** using a sequence classification head to directly map text responses to their corresponding scores.  \n",
    "2. **Apply Low-Rank Adaptation (LoRA)** to efficiently fine-tune the model, reducing the number of trainable parameters while preserving BERT’s generalization capabilities.  \n",
    "3. **Train the model** on labeled response data, leveraging fine-tuning to optimize classification performance while maintaining computational efficiency.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Building the datasets for huggingface trainer object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>response_corrected</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>During my journey, I faced a big health proble...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I once had a big problem when I wasn't ready f...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>During my junior year I faced a significant ch...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Once, I encountered a situation that was quite...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>During a trip, I suffered a serious accident t...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  response_corrected  score\n",
       "0  During my journey, I faced a big health proble...      3\n",
       "1  I once had a big problem when I wasn't ready f...      3\n",
       "2  During my junior year I faced a significant ch...      3\n",
       "3  Once, I encountered a situation that was quite...      2\n",
       "4  During a trip, I suffered a serious accident t...      3"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft_df = df[['response_corrected', 'score']]\n",
    "ft_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yw/d34pj4ns3r75s0xjlqh337j00000gn/T/ipykernel_71882/768332436.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ft_df['score'] = ft_df['score']-1\n"
     ]
    }
   ],
   "source": [
    "ft_df['score'] = ft_df['score']-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['response_corrected', 'score'],\n",
       "    num_rows: 1000\n",
       "})"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = Dataset.from_pandas(ft_df)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function_response(examples):\n",
    "    return tokenizer(\n",
    "        examples['response_corrected'],\n",
    "        padding='max_length',\n",
    "        truncation=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "297d1990305340049d2942054f8889bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dataset = dataset.map(tokenize_function_response, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['response_corrected', 'score', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "    num_rows: 1000\n",
       "})"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = tokenized_dataset.rename_column('score', 'labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tokenized_dataset.shuffle(seed=21).select(range(int(0.8 * len(tokenized_dataset))))\n",
    "test_dataset = tokenized_dataset.shuffle(seed=21).select(range(int(0.8 * len(tokenized_dataset)), len(tokenized_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['response_corrected', 'labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "    num_rows: 800\n",
       "})"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Loading the BERT model (sequenceclassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "model_path = \"google-bert/bert-base-uncased\"\n",
    "model_bert_classifier = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_path,\n",
    "    num_labels=5 # because from 1 to 5, we have 5 categories.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 Setup a lora configure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=8,               # Low-rank dimension\n",
    "    lora_alpha=32,      # Scaling factor\n",
    "    lora_dropout=0.1,   # Dropout rate\n",
    "    target_modules=[\"query\", \"key\"],  # Target attention weights\n",
    "    bias=\"none\",        # Bias type (\"none\", \"all\", or \"lora_only\")\n",
    "    task_type=\"SEQ_CLS\" # Task type (e.g., \"SEQ_CLS\" for sequence classification)\n",
    ")\n",
    "\n",
    "lora_model = get_peft_model(model_bert_classifier, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 298,757 || all params: 109,784,842 || trainable%: 0.2721\n"
     ]
    }
   ],
   "source": [
    "lora_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args_lora_ft = TrainingArguments(\n",
    "    output_dir=\"lora-fine-tuned-bert\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-4,  # Slightly higher learning rate for LoRA\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    #save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    #report_to=\"none\",\n",
    "    metric_for_best_model=\"accuracy\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=\"weighted\")\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\"accuracy\": acc, \"f1\": f1, \"precision\": precision, \"recall\": recall}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_lora_ft = Trainer(\n",
    "    model=lora_model,\n",
    "    args=training_args_lora_ft,\n",
    "    train_dataset=train_dataset,              # Training dataset\n",
    "    eval_dataset=test_dataset,                # Evaluation dataset\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1554c288f63245e7adcebb0808082683",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6379, 'grad_norm': 3.951148271560669, 'learning_rate': 0.0001866666666666667, 'epoch': 0.2}\n",
      "{'loss': 1.4587, 'grad_norm': 3.3521759510040283, 'learning_rate': 0.00017333333333333334, 'epoch': 0.4}\n",
      "{'loss': 1.3868, 'grad_norm': 3.232008218765259, 'learning_rate': 0.00016, 'epoch': 0.6}\n",
      "{'loss': 1.4262, 'grad_norm': 7.572861671447754, 'learning_rate': 0.00014666666666666666, 'epoch': 0.8}\n",
      "{'loss': 1.3526, 'grad_norm': 2.1482417583465576, 'learning_rate': 0.00013333333333333334, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81a763801ff84df59f5dde61ea124f9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.4103049039840698, 'eval_accuracy': 0.33, 'eval_f1': 0.1637593984962406, 'eval_precision': 0.10890000000000001, 'eval_recall': 0.33, 'eval_runtime': 245.8323, 'eval_samples_per_second': 0.814, 'eval_steps_per_second': 0.053, 'epoch': 1.0}\n",
      "{'loss': 1.3892, 'grad_norm': 3.005897283554077, 'learning_rate': 0.00012, 'epoch': 1.2}\n",
      "{'loss': 1.3955, 'grad_norm': 4.158362865447998, 'learning_rate': 0.00010666666666666667, 'epoch': 1.4}\n",
      "{'loss': 1.3298, 'grad_norm': 3.062124490737915, 'learning_rate': 9.333333333333334e-05, 'epoch': 1.6}\n",
      "{'loss': 1.4138, 'grad_norm': 5.912414073944092, 'learning_rate': 8e-05, 'epoch': 1.8}\n",
      "{'loss': 1.3497, 'grad_norm': 2.7183940410614014, 'learning_rate': 6.666666666666667e-05, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd5705fe902f4d2d8e1453e5badb5216",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.3777624368667603, 'eval_accuracy': 0.33, 'eval_f1': 0.1637593984962406, 'eval_precision': 0.10890000000000001, 'eval_recall': 0.33, 'eval_runtime': 3518.8656, 'eval_samples_per_second': 0.057, 'eval_steps_per_second': 0.004, 'epoch': 2.0}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[156], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer_lora_ft\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/trainer.py:1938\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1936\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1937\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1938\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1939\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1940\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1941\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1942\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1943\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/trainer.py:2279\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2276\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   2278\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 2279\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2281\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2282\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2283\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2284\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2285\u001b[0m ):\n\u001b[1;32m   2286\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2287\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/trainer.py:3318\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   3315\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   3317\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 3318\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3320\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[1;32m   3321\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   3322\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3323\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   3324\u001b[0m ):\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/trainer.py:3363\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   3361\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3362\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 3363\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3364\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   3365\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   3366\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/peft/peft_model.py:1379\u001b[0m, in \u001b[0;36mPeftModelForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[0m\n\u001b[1;32m   1377\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m peft_config\u001b[38;5;241m.\u001b[39mpeft_type \u001b[38;5;241m==\u001b[39m PeftType\u001b[38;5;241m.\u001b[39mPOLY:\n\u001b[1;32m   1378\u001b[0m             kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtask_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m task_ids\n\u001b[0;32m-> 1379\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1380\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1381\u001b[0m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1382\u001b[0m \u001b[43m            \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1383\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1384\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1385\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1386\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1387\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1388\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1390\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m _get_batch_size(input_ids, inputs_embeds)\n\u001b[1;32m   1391\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1392\u001b[0m     \u001b[38;5;66;03m# concat prompt attention mask\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/peft/tuners/tuners_utils.py:188\u001b[0m, in \u001b[0;36mBaseTuner.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any):\n\u001b[0;32m--> 188\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:1695\u001b[0m, in \u001b[0;36mBertForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1687\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1688\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1689\u001b[0m \u001b[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m   1690\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[1;32m   1691\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m   1692\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1693\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1695\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1696\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1697\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1698\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1699\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1700\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1701\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1702\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1703\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1704\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1705\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1707\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   1709\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(pooled_output)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:1141\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1134\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[1;32m   1135\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[1;32m   1136\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[1;32m   1137\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[1;32m   1138\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[1;32m   1139\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m-> 1141\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1142\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1143\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1145\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1146\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1148\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1149\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1150\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1151\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1152\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1153\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1154\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:694\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    683\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    684\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    685\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    691\u001b[0m         output_attentions,\n\u001b[1;32m    692\u001b[0m     )\n\u001b[1;32m    693\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 694\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    695\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    699\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    700\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    701\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    704\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:626\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    623\u001b[0m     cross_attn_present_key_value \u001b[38;5;241m=\u001b[39m cross_attention_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    624\u001b[0m     present_key_value \u001b[38;5;241m=\u001b[39m present_key_value \u001b[38;5;241m+\u001b[39m cross_attn_present_key_value\n\u001b[0;32m--> 626\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m \u001b[43mapply_chunking_to_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed_forward_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunk_size_feed_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseq_len_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    629\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (layer_output,) \u001b[38;5;241m+\u001b[39m outputs\n\u001b[1;32m    631\u001b[0m \u001b[38;5;66;03m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/pytorch_utils.py:239\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[1;32m    237\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(output_chunks, dim\u001b[38;5;241m=\u001b[39mchunk_dim)\n\u001b[0;32m--> 239\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:638\u001b[0m, in \u001b[0;36mBertLayer.feed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    637\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n\u001b[0;32m--> 638\u001b[0m     intermediate_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintermediate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    639\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(intermediate_output, attention_output)\n\u001b[1;32m    640\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:538\u001b[0m, in \u001b[0;36mBertIntermediate.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 538\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    539\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintermediate_act_fn(hidden_states)\n\u001b[1;32m    540\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer_lora_ft.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fb2e78da6304d47805440f5f65398c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: 2, Actual: 4\n",
      "Prediction: 2, Actual: 1\n",
      "Prediction: 2, Actual: 3\n",
      "Prediction: 2, Actual: 4\n",
      "Prediction: 2, Actual: 2\n",
      "Prediction: 2, Actual: 3\n",
      "Prediction: 2, Actual: 3\n",
      "Prediction: 2, Actual: 2\n",
      "Prediction: 2, Actual: 4\n",
      "Prediction: 2, Actual: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Predict on the test dataset\n",
    "predictions = trainer_lora_ft.predict(test_dataset)\n",
    "\n",
    "# The predictions will contain the predicted labels, logits, and labels\n",
    "predicted_labels = predictions.predictions.argmax(-1)\n",
    "\n",
    "# If you want to see the predictions alongside the actual labels\n",
    "actual_labels = predictions.label_ids\n",
    "\n",
    "# You can print the first few predictions and the corresponding true labels\n",
    "for i in range(10):\n",
    "    print(f\"Prediction: {predicted_labels[i]}, Actual: {actual_labels[i]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
